\documentclass[11pt]{book}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{multirow}

\newtheoremstyle{example} % Style name
	{10pt} % Space above
	{10pt} % Space below
	{\normalfont} % Body font
	{} % Header font
	{\bfseries} % Header font
	{.} % Separator
	{10pt} % Space after header
	{} % Header text
\theoremstyle{example}
\newtheorem{example}{Example}[chapter]

\newtheoremstyle{problem} % Style name
	{10pt} % Space above
	{10pt} % Space below
	{\normalfont} % Body font
	{} % Header font
	{\bfseries} % Header font
	{.} % Separator
	{10pt} % Space after header
	{} % Header text
\theoremstyle{problem}
\newtheorem{problem}{Problem}[chapter]

\newtheoremstyle{theorem} % Style name
	{10pt} % Space above
	{10pt} % Space below
	{\normalfont} % Body font
	{} % Header font
	{\bfseries} % Header font
	{.} % Separator
	{10pt} % Space after header
	{} % Header text
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}[chapter]

\begin{document}

\title{Partial Differential Equations}
\author{Edited by Stewart Nash}
\date{2026}

\frontmatter

\maketitle

\chapter*{Preface}
This material is taken from ``Schaum's Outline of Theory and Problems of Partial Differential Equations'' by Paul DuChateau and David W. Zachmann. The footnotes are taken from content gathered from Wikipedia.

\tableofcontents

\mainmatter

\chapter[DM for Parabolic Equations]{Difference Methods for Parabolic Equations}

\section{Difference Equations}
%The various partial derivatives of a function $u(x,t)$ can be expressed as a \emph{difference quotient} plus a \emph{truncation error} $\mathcal{O}(k)$.
The partial derivatives of a sufficiently smooth function $u(x,t)$ may be approximated by \emph{finite difference quotients}. Each such approximation consists of a discrete difference operator plus a \emph{truncation error}, which arises from neglecting higher-order terms in the Taylor expansion. 
\subsection{Forward difference approximation of $u_t$}
Using Taylor's theorem in time, we obtain the following approximation for $u_t$ and expression for the truncation error $R_t$
\begin{align}
	u_t(x,t)&=\frac{u(x,t+k)-u(x,t)}{k}+R_t(x,t),\\
	R_t(x,t)&=-\frac{k}{2}u_{tt}(x,\bar{t}),\qquad{t<\bar{t}<t+k}.
	\tag{Forward Difference for $u_t$}
\end{align}
If $u_{tt}$ is bounded, then
\begin{equation*}
	R_t(x,t)=\mathcal{O}(k)\qquad(k\rightarrow0)
\end{equation*}
\subsection{Centered difference approximation of $u_x$}
Expanding in space about $x$, we obtain
\begin{align}
	u_x(x,t)&=\frac{u(x+h,t)-u(x-h,t)}{2h}+R_x(x,t),\\
	R_x(x,t)&=-\frac{h^2}{6}u_{xxx}(\bar{x},t),\qquad{x-h<\bar{x}<x+h}.
	\tag{Centered Difference for $u_x$}
\end{align}
If $u_{xxx}$ is bounded, then
\begin{equation*}
	R_x(x,t)=\mathcal{O}(h^2)\qquad(h\rightarrow0)
\end{equation*}
\subsection{Centered difference approximation of $u_{xx}$}
The second spatial derivative is approximated by
\begin{align}
	u_{xx}(x,t)&= \frac{u(x+h,t)-2u(x,t)+u(x-h,t)}{h^2}+R_{xx}(x,t),\\
	R_{xx}(x,t)&=-\frac{h^2}{12}u_{xxxx}(\bar{x},t),\qquad{x-h<\bar{x}<x+h}.
	\tag{Centered Difference for $u_{xx}$}
\end{align}
If $u_{xxxx}$ is bounded, then
\begin{equation*}
	R_{xx}(x,t)=\mathcal{O}(h^2)\qquad(h\rightarrow0)
\end{equation*}
\subsection{Centered difference approximation of $u_{xt}$}
A centered approximation to $u_{xt}$ is given by
%\begin{align}
%	u_{xt}(x,t)&= \frac{u(x+h,t+k)-u(x+h,t-k)-u(x-h,t+k)+u(x-h,t-k)}{4hk} + \mathcal{O}(h),\\
%	\mathcal{O}(h^2,k^2)&\equiv-\frac{h^2}{6}u_{xxxt}(\bar{x},\bar{t})-\frac{k^2}{6}u_{xttt}(\tilde{x},\tilde{t}),\qquad{x-h<\bar{x},\tilde{x}<x+h,t-k<\bar{t},\tilde{t}<t+k}.
%	\tag{Centered Difference for $u_{xt}$}
%\end{align}
\begin{equation}
	u_{xt}(x,t)=\frac{1}{4hk}
	\begin{aligned}[t]
		\bigl(&u(x+h,t+k)-u(x+h,t-k)\\
		&-u(x-h,t+k)+u(x-h,t-k)\bigr)
	\end{aligned}
	+R_{xt}(x,t)
	\tag{Centered Difference for $u_{xt}$}
\end{equation}
The truncation error has the form
\begin{equation}
	R_{xt}(x,t)=-\frac{h^2}{6}u_{xxxt}(\bar{x},\bar{t})-\frac{k^2}{6}u_{xttt}(\tilde{x},\tilde{t})
\end{equation}
where
\begin{equation*}
	x-h<\bar{x},\tilde{x}<x+h,{\qquad}t-k<\bar{t},\tilde{t}<t+k
\end{equation*}
If $u_{xxxt}$ and $u_{xttt}$ are bounded, then
\begin{equation*}
	R_{xt}(x,t)=\mathcal{O}(h^2+k^2)\qquad(h,k\rightarrow0).
\end{equation*}
\subsection{O-notation}
%Usually it is only the order of magnitude of the truncation error which is of interest. A function $f(h)$ is said to be of the \emph{order of magnitude $g(h)$ as $h\rightarrow0$}, where $g$ is a nonnegative function, if
%\begin{equation*}
%	\lim\limits_{h\rightarrow0}\frac{f(h)}{g(h)}=\mathrm{constant}
%\end{equation*}
%In the \emph{$\mathcal{O}$-notation} we write $f(h)=\mathcal{O}(g(h))\,(h\rightarrow0)$. It is easy to see that if, as $h_1,h_2\rightarrow0$,$f_1=\mathcal{O}(g_1)$ and $f_2=\mathcal{O}(g_2)$, then $f_1+f_2=\mathcal{O}(g_1+g_2)$.
Usually it is only the order of magnitude of the truncation error which is of interest. A function $f(h)$ is said to be of the \emph{order $g(h)$ as $h\rightarrow0$}, if there exist constants $C>0$ and $h_0>0$ such that
\begin{equation*}
	|f(h)|{\leq}C|g(h)|,\qquad0<|h|<h_0\mathrm{.}
\end{equation*}
In this case, we write
\begin{equation*}
	f(h)=\mathcal{O}(g(h))\qquad(h\rightarrow0)\mathrm{.}
\end{equation*}
If, as $h_1,h_2\rightarrow0$,$f_1=\mathcal{O}(g_1)$ and $f_2=\mathcal{O}(g_2)$, then
\begin{equation*}
	f_1+f_2=\mathcal{O}(g_1+g_2)\mathrm{.}
\end{equation*}
\begin{example}
	For (9.1), the truncation error is $\mathcal{O}(k)\,(k\rightarrow0)$, provided $u_{tt}$ is bounded. For (9.2), the truncation error is $\mathcal{O}(h^2)\,(h\rightarrow0)$, provided $u_{xxx}$ is bounded. For (9.4), the truncation error is $\mathcal{O}(h^2+k^2)\,(h\rightarrow0)$, provided $u_{xxxt}$ and $u_{xttt}$ are bounded; we omit as understood the limits $h\rightarrow0$ and $k\rightarrow0$.
\end{example}

\chapter[DM for Hyperbolic Equations]{Difference Methods for Hyperbolic Equations}

\section{One-Dimensional Wave Equation}
Methods similar to those given in Section 9.4 may be used to approximate smooth solutions to
\begin{equation}
	u_{tt}=c^2u_{xx}
\end{equation}
Let $(x_n,t_j)=(nh,jk)$ $(n,j=0,1,2,\dots)$ and write $s{\equiv}k/h$; we have as representatives of the two sorts of methods:

\chapter[Variational Formulation of BVP]{Variational Formulation of Boundary Value Problems}
\section{Introduction}
In certain cases, the solution of a boundary value problem for a PDE is also a solution of an associated calculus of variations problem. A typical problem in the calculus of variations is to find, for a function $u$ belonging a prescribed set $\mathscr{A}$, the extreme values of the integral expression
\begin{equation}
	J[u(\mathbf{x})]=\int_\Omega{F(\mathbf{x},u(\mathbf{x}),\mathbf{\nabla}u(\mathbf{x}))\,d\Omega}
\end{equation}
where $F$ denotes a given function. Hence we shall begin by describing some of the structure of the domain $\mathscr{A}$ of $J$.

\section{The Function Space $L^2(\Omega)$}
In Chapter 6 we considered the space $L^2(a,b)$ of functions $f(x)$ that are defined and square integrable on $(a,b)$ in $\mathbb{R}^1$. More generally, let let $\Omega$ denote a bounded region in $\mathbb{R}^n$ and consider the set $L^2(\Omega)$ of all real-valued functions $u(\mathbf{x})$ defined on $\Omega$ which satisfy
\begin{equation}
	\int_\Omega{u^2(\mathbf{x})\,d\Omega}<\infty
\end{equation}
Like $L^2(a,b)$, $L^2(\Omega)$ is a vector space over the real numbers, and the expected definition
\begin{equation}
	\langle{u,v}\rangle\equiv\int_\Omega{u(\mathbf{x})v(\mathbf{x})\,d\Omega}
\end{equation}
makes it an inner product space.\\
A subset of $L^2(\Omega)$ is said to be a \emph{subspace} of $L^2(\Omega)$ if the subset is closed under the operation of forming linear combinations.
\begin{example}
	(a) For $k$ a nonnegative integer, the subset $C^k(\bar{\Omega})$ of all $u$ in $L^2(\Omega)$ which, together with all derivatives of order $k$ or less, are continuous on $\bar{\Omega}$ is a subspace of $L^2(\Omega)$. (b) Let $u_1,\dots,u_N$ denote $N$ elements of $L^2(\Omega)$. The subset $\mathscr{M}$ of all linear combinations of $u_1,\dots,u_N$ is a subspace of $L^2(\Omega)$---the subspace \emph{spanned} by the $u_i$. (c) For $m$ a positive integer, the subset $H^m(\Omega)$ of all $u$ in $L^2(\Omega)$ whose derivative of order $m$ or less are also in $L^2(\Omega)$ is a subspace of $L^2(\Omega)$.
\end{example}
A subspace $\mathscr{M}$ of $L^2(\Omega)$ is \emph{dense in} $L^2(\Omega)$ if for any $\epsilon>0$ and any $f$ in $L^2(\Omega)$ there exists a $v$ in $\mathscr{M}$ such that
\begin{equation}
	\|v-f\|^2=\int_\Omega{(v-f)^2\,d\Omega}<\epsilon
\end{equation}
i.e., if any $f$ in $L^2(\Omega)$ can be approximated with arbitrary precision in the least-squares sense by a function from $\mathscr{M}$.
\begin{theorem}
	For each positive integer $m$, the following subspaces are dense in $L^2(\Omega)$: $C^m(\bar{\Omega})$, $H^m(\Omega)$, and the set of all $u$ in $C^m(\bar{\Omega})$ such that $u=0$ on $S$, the boundary of $\Omega$.
\end{theorem}
\begin{theorem}
	If $\mathscr{M}$ is a dense subspace in $L^2(\Omega)$ and if an element $u$ of $L^2(\Omega)$ satisfies $\langle{u,v}\rangle=0$ for all $v$ in $\mathscr{M}$, then $u=0$.
\end{theorem}
	
\chapter{Variational Approximation Methods}
This chapter presents some techniques for constructing approximate solutions to boundary value problems. These techniques are based on the ideas of Chapter 12 and differ markedly from the finite-difference methods of Chapters 9, 10, and 11.

\section{The Rayleigh-Ritz Procedure}
This approximation procedure is limited to boundary value problems admitting the variational formulation ``Find $u*$ in $\mathscr{A}$ such that functional $J[u]$ is minimized over $\mathscr{A}$ by $u*$.'' It was seen in Chapter 12 that such boundary value problems arise in connection with self-adjoint elliptic PDEs.\\
We suppose $\mathscr{A}$ to be some subset of $L^2(\Omega)$, where $\Omega$ denotes a bounded set in $\mathbb{R}^n$ with smooth boundary $S$ consisting of complementary portion $S_1$ and $S_2$. Specifically, we take $\mathscr{A}=\{u\,\mathrm{in}\,H^1(\Omega):\,u=g\,\mathrm{on}\,S_1\}$ for a given $g$ in $C(\bar{\Omega}$; the associated subspace of comparison functions is taken as $\mathscr{M}=\{v\,\mathrm{in}\,H^1(\Omega):v=0\,\mathrm{on}\,S_1\}$.\\
Let $\phi_0$ denote an arbitrary function from $\mathscr{A}$ (e.g., $\phi_0=g$) and let $\phi_1,\dots,\phi_N$ denote $N$ linearly independent functions $\mathscr{M}$. Then,
\begin{equation}
	u_N(\mathbf{x})\equiv\phi_0(\mathbf{x})+\sum_{j=1}^N{c_j\phi_j(\mathbf{x})}
\end{equation}
belongs to $\mathscr{A}$ for \emph{all} choices of the constants $c_1,\dots,c_N$; we denote by $\mathscr{A}_N$ the subset of $\mathscr{A}$ consisting of all such functions $u_N$. Let $u_N^*$ denote the function in $\mathscr{A}_N$ that minimizes $J$ over $\mathscr{A}_N$. It can be shown that $u_N^*$ represents the best approximation, in the least-squares sense, from $\mathscr{A}_N$ to the exact solution $u^*$. This function $u_N^*$ is called the \emph{Rayleigh-Ritz approximation} to the solution of the boundary value problem.\\
The minimization of $J$ over $\mathscr{A}_N$ is tantamount to the minimization over all $\mathbf{c}$ in $\mathbb{R}^N$ of the ordinary function
\begin{equation}
	H(c_1,\dots,c_N){\equiv}J\left[\phi_0+\sum_{j=1}^N{c_j\phi_j}\right]
\end{equation}
The minimizing constants $c_1^*,c_2^*,\dots,c_N^*$ must satisfy
\begin{equation}
	\frac{{\partial}H}{{\partial}c_m}(c_1,\dots,c_N)=0\quad(m=1,\dots,N)
\end{equation}
which is a system of $N$ equations in $N$ unknowns.\\
The Rayleigh-Ritz procedure may also be applied to eigenvalue problems of the sort treated in Section 12.4. In any eigenvalue problem the boundary conditions are all homogeneous. Hence, we take $\phi_0{\equiv}0$ in (13.1) and minimize the Rayleigh quotient $J$ over the subspace $\mathscr{A}_N=\mathscr{M}_N$.

\section{The Galerkin Procedure}
This approximation method is employed when the boundary value problem admits only a weak formulation; e.g., in the case of a linear elliptic PDE containing odd-order derivates. In the event that conditions (12.24) hold in the weak formulation, so that a variational formulation also exists, it can be shown (see Problem 13.4(b)) that the Galerkin and Rayleigh-Ritz procedure coincide.\\
Consider, then, a boundary value problem with the weak formulation ``Find $u^*$ in $\mathscr{A}$ such that $K[u^*,v]=F[v]$ for all $v$ in $\mathscr{M}$.'' Here we suppose that $\Omega$, $S$, $\mathscr{A}$, and $\mathscr{M}$ are as described in Section 13.1. Let $phi_1,\dots,\phi_N$ denote $N$ linearly independent \emph{trial functions} in $\mathscr{M}$ and let $\phi_0$ denote an arbitrary function in $\mathscr{A}$. As in the Rayleigh-Ritz procedure, we seek an approximation $u_N^*$ to the weak solution $u^*$ of the form (13.1). In addition, let $\psi_1,\dots,\psi_N$ denote $N$ linearly independent \emph{weight functions} in $\mathscr{M}$; these may or may not be the same as the trail functions. The Galerkin approximate solution is required to satisfy
\begin{equation}
	K[u_N^*,\psi_j]=F[\psi_j]\quad(j=1,\dots,N)
\end{equation}
This is a set of $N$ equations in the $N$ unknowns $c_1^*,\dots,c_N^*$.

\chapter[FEM Introduction]{The Finite Element Method: An Introduction}
The success of the approximation methods presented in Chapter 13 is largely dependent on the selection of an effective collection of trial functions $\phi_j$ and/or weight functions $\psi_j$. If these functions are chosen from certain families of piecewise polynomials, called \emph{finite element spaces}, the following advantages are realized:
\begin{enumerate}[(i)]
	\item It is possible to deal in a systematic fashion with regions $\Omega$ having curved boundaries of rather arbitrary shape.
	\item One can systematically estimate the accuracy of the approximate solution in terms of the adjustable parameters associated with the finite element family.
	\item The coefficient matrix and data vector for the system of algebraic equations defining the approximate solution can be efficiently generated by computer.
\end{enumerate}

\section{Finite Element Spaces in One Dimension}
Suppose that the interval $[0,1]$ is subdivided into $N$ subintervals each of length $h=1/N$. Let $x_j=jh$ $(j=0,1,\dots,N)$ denote the nodes in the interval $[0,1]$. Then the finite element space denoted by $S^h[k,r]$ shall consist all functions $\phi(x)$ defined on $[0,1]$ such that (i) on each subinterval $[x_j,x_{j+1}]$, $\phi(x)$ is a polynomial of degree at most $k$; (ii) $\phi(x)$ has $r$ continuous derivatives on $[0,1]$, which is to say, $\phi$ belongs to $C'[0,1]$.\\
If $r=0$, $\phi$ is continuous but not necessarily differentiable at nodes. If $\phi$ is to be allowed to be discontinuous at nodes, we set $r=-1$. Evidently, $S^h[k,r]$ is a finite-dimensional vector space (a subspace of $L^2(0,1)$) and so may be characterized by giving a \emph{basis}; i.e., a linearly independent set of elements $\{\phi_j\}$ that spans the space.\\
Modifications for the case of nonuniform grids are easily developed.
\begin{example}
	A basis for $S^h[0,-1]$, the \emph{piecewise constants}, is given by
	\begin{equation}
		\phi_j(x)=
		\begin{cases}
			1 & x_{j-1}{\leq}x{\leq}x_j\\
			0 & \text{otherwise}
		\end{cases}	
	\end{equation}
	for $j=1,2,\dots,N$ $(Nh=1)$. See Fig. 14-1. The functions in $S^h[0,-1]$ are in $L^2(0,1)$ but are not continuous. 
\end{example}

\end{document}
